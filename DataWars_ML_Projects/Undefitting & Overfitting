Overfitting is a common problem in machine learning, where a model performs well on training data but does not genealize well to unseen data (test data). In this cases, we also said that the model has a high variance.

High variance models are characterized by having many parameters and a complex structure.

Other common problem is when the model suffer from underfitting (High bias). These models are characterized by having few parameters and a simple structure, which can't adapt to the complexity of the problem.

These models tend to have similar performance on both training and testing data, but they are not able to capture the underlying patterns resulting in a high error on both sets.


Preview
Photo by geeksforgeeks

If we consider that we have the ability to adjust the complexity of the model, we anticipate that the performance of the model on the training data (training score) and its performance on unseen data (validation score) would exhibit the behavior depicted in the following figure:


Preview
Based on the accuracy values obtained, we can draw the following conclusion:

"For models with high bias, the performance on the validation set is similar to the performance on the training set."

"For models with high variance, the performance on the validation set is significantly different from the performance on the training set."

The diagram shown here is often called a validation curve.

A validation curve is a diagnostic tool used to evaluate the performance of a classifier for different values of a hyperparameter.

Remember that a hyperparameter is a parameter that is set before training a model, unlike model parameters that are learned during training.

For example, in decision trees, the maximum depth of the tree is a hyperparameter that can be adjusted to control the complexity of the model. In a validation curve, the performance of the model is evaluated for different values of the maximum depth using a validation set. A validation curve can help in identifying the optimal hyperparameter value that balances underfitting and overfitting.

Underfitting and overfitting are common issues in decision tree models.
Underfitting and overfitting problems
Underfitting occurs when the decision tree is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and validation data. This often happens when the tree is too shallow, has few branches, or has a small maximum depth limit.

Overfitting occurs when the decision tree is too complex and fits the training data too well, capturing even the noise and random fluctuations in the data. This results in an excellent performance on the training data but poor performance on validation and new data. This often happens when the tree is too deep, has too many branches, or has a large maximum depth limit.

Toy dataset
To understand better these concepts we are going to use a validation plot using a toy dataset. The Iris dataset is a popular multi-class classification dataset for machine learning. It consists of 150 samples of iris flowers, with 50 samples from each of three species: Iris setosa, Iris virginica, and Iris versicolor.

For each sample, four features were measured: the sepal length, sepal width, petal length, and petal width. These features are used to classify the samples into one of the three species of iris.

Before focusing on the validation curve, let's check the dataset following this instruction:

1- Load the dataset in the notebook: Start by loading the iris dataset into your programming environment. The iris dataset is commonly used in machine learning and is available in seaborn.

2- Check the structure of the data by inspecting the number of rows, columns, and attributes of the dataset. You can use functions like .shape, .head(), .info(), and .describe() to get a quick overview of the data.

3- Check for missing or duplicate values and clean the data if necessary. You can use functions like .isnull().sum() or .drop_duplicates() to detect and remove missing or duplicate values.

4- Visualize the data using different plots to gain insights into the distribution and relationships between variables.

5- Calculate summary statistics such as mean, median, mode, and standard deviation for each variable to understand the central tendency and variability of the data.

6- Identify the most relevant features for your analysis by considering the correlation between features and the target variable.

Based on previous tasks, let's check your knowledge:

1
Did you find any missing value?


No, data does not contain missing values


Yes, therea are more than 5 missing values

Correct!
2
Visualize the data using a pairplot to gain insights into the distribution and relationships between variables.

Store the figure in the variable p

Correct!
3
What is the mean of 'Sepal.Length' for the species virginica?


5.552


2.974


6.588


2.026

Correct!
4
What is the correlation between 'Sepal.Length' and 'Petal_width'?


-0.117


-0.36


0.817

Correct!
Underfitting and overfitting are common issues in decision tree models.
Validation curve
For the decision tree, the max_depth parameter is used to control the tradeoff between under-fitting and over-fitting. Now we will show the validation curve by plotting the training and testing errors.

The following code trains a decision tree classifier with different maximum depths and evaluates its performance on a validation set. The accuracy scores are stored in a list and plotted against the maximum depth values to show how the accuracy changes as the complexity of the model increases.

The optimal maximum depth is usually at the point where the accuracy is highest.

Let's run this code in the notebook

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load your dataset
iris = pd.read_csv('iris.csv')

X=iris.drop(columns='species')
y=iris.species

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3,random_state=0)

# Define a list of hyperparameter values to evaluate
max_depths = range(1, 20)

# Create an empty list to store the accuracy scores
accuracy_scores_train = []
accuracy_scores_test = []

# Loop over the hyperparameter values
for max_depth in max_depths:
    # Train a decision tree with the current hyperparameter value
    model = DecisionTreeClassifier(max_depth=max_depth)
    model.fit(X_train, y_train)


    # Evaluate the model on the training set
    y_pred = model.predict(X_train)
    accuracy_train = accuracy_score(y_train, y_pred)

    # Evaluate the model on the validation set
    y_pred = model.predict(X_val)
    accuracy_test = accuracy_score(y_val, y_pred)

    # Add the accuracy score to the list
    accuracy_scores_test.append(accuracy_test)
    accuracy_scores_train.append(accuracy_train)

# Plot the validation curve
plt.plot(max_depths, accuracy_scores_test, label='Test')
plt.plot(max_depths, accuracy_scores_train, label='Train')
plt.xlabel("Maximum Depth")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Validation Curve for Decision Tree")
plt.show()

Preview
Now answer the following question.

5
What is the optimal maximum depth for the previous example


2.5


5


3


17

Correct!
Regularization parameter in Logistic Regression
In logistic regression, the C coefficient is a parameter in the regularization term of the cost function. The regularization term helps to prevent overfitting by adding a penalty term to the cost function that discourages large values of the model's coefficients. The C coefficient is used to control the strength of the regularization term.

The C parameter in scikit-learn is the inverse of regularization strength.

A smaller value of C will result in a stronger regularization, which will shrink the parameters towards zero and make the model more constrained.

A larger value of C will result in a weaker regularization, which will allow the parameters to take on larger values and make the model more flexible.

For example, in logistic regression, the cost function is defined as:


where 
, so a smaller value of C corresponds to a larger value of 
 and a stronger regularization.

Let's run the following examples in the notebook:

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from mlxtend.plotting import plot_decision_regions

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           n_clusters_per_class=2, random_state=0)

# C = 1
clf1 = LogisticRegression(C=100, random_state=0)
clf1.fit(X, y)

# C = 0.1
clf2 = LogisticRegression(C=1, random_state=0)
clf2.fit(X, y)

# C = 0.01
clf3 = LogisticRegression(C=0.01, random_state=0)
clf3.fit(X, y)


fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(10,5))
# Plotting decision regions
plot_decision_regions(X ,y, clf=clf1, legend=2,ax=ax1)
plot_decision_regions(X ,y, clf=clf2, legend=2,ax=ax2)
plot_decision_regions(X ,y, clf=clf3, legend=2,ax=ax3)
# Adding axes annotations
ax1.set_title('C=10')
ax2.set_title('C=1')
ax3.set_title('C=0.1')
plt.show()

Preview
Then answer the following question.

6
Three different models are trained, each with a different value of the C parameter: C=1, C=0.1, and C=0.01. Determine which one of the boundaries belongs to each C value.


The higher the value of C, the stronger the regularization, and the higher the values of the model's coefficients will be.


The smaller the value of C, the stronger the regularization, and the smaller the values of the model's coefficients will be.

Correct!
Conclusion
In this lab, we learned about two key concepts: Overfitting and Underfitting. We demonstrated an example of how to select the optimal hyperparameter of a Decision Tree model by using a loop. In future labs, we will explore additional performance metrics, cross-validation techniques, and various hyperparameters.

Additionally, we learned that in Logistic Regression, the 'C' parameter controls the strength of regularization. The decision boundary in Logistic Regression is defined as the line where the predicted probability of the positive class is equal to 0.5. The shape and location of the decision boundary can be influenced by the value of the C parameter.


 Project: Overfitting and underfitting
## Project: Overfitting and underfitting
Importing required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_blobs
from mlxtend.plotting import plot_decision_regions
1- Load the dataset: Start by loading the iris dataset into your programming environment. The iris dataset is a commonly used dataset in machine learning and is available in many libraries such as scikit-learn and seaborn.

iris = pd.read_csv('iris.csv')
iris.head()
sepal_length	sepal_width	petal_length	petal_width	species
0	5.1	3.5	1.4	0.2	setosa
1	4.9	3.0	1.4	0.2	setosa
2	4.7	3.2	1.3	0.2	setosa
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa
2- Check the structure of the data by inspecting the number of rows, columns, and attributes of the dataset. You can use functions like .shape, .head(), .info(), and .describe() to get a quick overview of the data.

print("Shape of the dataset (rows, columns):")
print(iris.shape)
​
# Display the first few rows of the dataset
print("\nFirst five rows of the dataset:")
print(iris.head())
​
# Display dataset information (data types and non-null values)
print("\nDataset information:")
print(iris.info())
​
# Display summary statistics for numerical columns
print("\nSummary statistics for numerical columns:")
print(iris.describe())
​
Shape of the dataset (rows, columns):
(150, 5)

First five rows of the dataset:
   sepal_length  sepal_width  petal_length  petal_width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa

Dataset information:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 150 entries, 0 to 149
Data columns (total 5 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   sepal_length  150 non-null    float64
 1   sepal_width   150 non-null    float64
 2   petal_length  150 non-null    float64
 3   petal_width   150 non-null    float64
 4   species       150 non-null    object 
dtypes: float64(4), object(1)
memory usage: 6.0+ KB
None

Summary statistics for numerical columns:
       sepal_length  sepal_width  petal_length  petal_width
count    150.000000   150.000000    150.000000   150.000000
mean       5.843333     3.057333      3.758000     1.199333
std        0.828066     0.435866      1.765298     0.762238
min        4.300000     2.000000      1.000000     0.100000
25%        5.100000     2.800000      1.600000     0.300000
50%        5.800000     3.000000      4.350000     1.300000
75%        6.400000     3.300000      5.100000     1.800000
max        7.900000     4.400000      6.900000     2.500000
3- Check for missing or duplicate values and clean the data if necessary. You can use functions like .isnull().sum() or .drop_duplicates() to detect and remove missing or duplicate values.

# Check for missing values
print("Checking for missing values:")
missing_values = iris.isnull().sum()
print(missing_values)
​
# Check for duplicate rows
print("\nChecking for duplicate rows:")
duplicate_rows = iris.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_rows}")
​
# Clean the data
# Drop duplicate rows
iris_cleaned = iris.drop_duplicates()
​
# Display the shape of the cleaned dataset
print("\nShape of the dataset after removing duplicates:")
print(iris_cleaned.shape)
​
# Verify if there are still any missing or duplicate values
print("\nRechecking for missing values after cleaning:")
print(iris_cleaned.isnull().sum())
​
print("\nRechecking for duplicate rows after cleaning:")
print(f"Number of duplicate rows: {iris_cleaned.duplicated().sum()}")
​
Checking for missing values:
sepal_length    0
sepal_width     0
petal_length    0
petal_width     0
species         0
dtype: int64

Checking for duplicate rows:
Number of duplicate rows: 1

Shape of the dataset after removing duplicates:
(149, 5)

Rechecking for missing values after cleaning:
sepal_length    0
sepal_width     0
petal_length    0
petal_width     0
species         0
dtype: int64

Rechecking for duplicate rows after cleaning:
Number of duplicate rows: 0
4- Visualize the data using different plots to gain insights into the distribution and relationships between variables. You can use functions like sns.pairplot(), sns.histplot(), sns.boxplot(), and sns.scatterplot() to visualize the data.

p= ...
​
Visualizing relationships between variables with pairplot:

Distribution of features using histograms:




Boxplot of features grouped by species:

Scatterplot: Sepal Length vs Sepal Width:

Scatterplot: Petal Length vs Petal Width:

5- Calculate summary statistics such as mean, median, mode, and standard deviation for each variable to understand the central tendency and variability of the data.

p=sns.pairplot(data=iris, hue='species')
plt.show()
​
​
6- Identify the most relevant features for your analysis by considering the correlation between features and the target variable.

# Calculate the correlation matrix for numerical features
correlation_matrix = iris.corr(numeric_only=True)
​
print("Correlation matrix for numerical features:")
print(correlation_matrix)
​
# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix for Numerical Features")
plt.show()
​
Correlation matrix for numerical features:
              sepal_length  sepal_width  petal_length  petal_width
sepal_length      1.000000    -0.117570      0.871754     0.817941
sepal_width      -0.117570     1.000000     -0.428440    -0.366126
petal_length      0.871754    -0.428440      1.000000     0.962865
petal_width       0.817941    -0.366126      0.962865     1.000000

Underfitting and overfitting are common issues in decision tree models.
7- What is the optimal maximum depth for this example ?

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
​
X=iris.drop(columns='species')
y=iris.species
​
# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3,random_state=0)
​
# Define a list of hyperparameter values to evaluate
max_depths = range(1, 20)
​
# Create an empty list to store the accuracy scores
accuracy_scores_train = []
accuracy_scores_test = []
​
# Loop over the hyperparameter values
for max_depth in max_depths:
    # Train a decision tree with the current hyperparameter value
    model = DecisionTreeClassifier(max_depth=max_depth)
    model.fit(X_train, y_train)
    
​
    # Evaluate the model on the training set
    y_pred = model.predict(X_train)
    accuracy_train = accuracy_score(y_train, y_pred)
​
    # Evaluate the model on the validation set
    y_pred = model.predict(X_val)
    accuracy_test = accuracy_score(y_val, y_pred)
    
    # Add the accuracy score to the list
    accuracy_scores_test.append(accuracy_test)
    accuracy_scores_train.append(accuracy_train)
​
# Plot the validation curve
plt.plot(max_depths, accuracy_scores_test, label='Test')
plt.plot(max_depths, accuracy_scores_train, label='Train')
plt.xlabel("Maximum Depth")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Validation Curve for Decision Tree")
plt.show()
​
